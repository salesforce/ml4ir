architecture_key: dnn
layers:
  - type: dense
    name: first_dense
    units: 256
    activation: relu
  - type: dropout
    name: first_dropout
    rate: 0.0
  - type: dense
    name: second_dense
    units: 64
    activation: relu
  - type: dropout
    name: second_dropout
    rate: 0.0
  - type: dense
    name: final_dense
    units: 1
    activation: null
optimizer:
  key: sgd
  gradient_clip_value: 5.0       #default value is 5.0
lr_schedule:
  key: cyclic
  type: triangular2
  initial_learning_rate: 0.001   #default value is 0.001
  maximal_learning_rate: 0.1    #default value is 0.01
  step_size: 5                  #default value is 10



#Below is an example of defining other learning rate schedules.
#
#Cyclic Learning Rate Schedule
#optimizer:
#  key: sgd
#  gradient_clip_value: 5.0       #default value is 5.0
#lr_schedule:
#  key: cyclic
#  type: triangular
#  initial_learning_rate: 0.001   #default value is 0.001
#  maximal_learning_rate: 0.01    #default value is 0.01
#  step_size: 10                  #default value is 10
#
#Exponential Decayed Learning Rate Schedule
#optimizer:
#  key: sgd
#  gradient_clip_value: 5.0       #default value is 5.0
#lr_schedule:
#  key: exponential
#  learning_rate: 0.01                   #default value is 0.01
#  learning_rate_decay_steps: 100000   #default value is 100000
#  learning_rate_decay: 0.96              #default value is 0.96
